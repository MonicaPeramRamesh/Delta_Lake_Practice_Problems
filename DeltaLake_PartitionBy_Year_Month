Create Delta table from DataFrame with partitioning (year, month)

from pyspark.sql.types import *
from pyspark.sql.functions import *

df = spark.read.format("csv").option("header","true").load("/Volumes/delta_practice_catalog/actual_files/tests/users.csv")

df = df.withColumn("date", to_date(col("purchase_timestamp"), "yyyy-MM-dd HH:mm:ss")).withColumn("year", year("date")).withColumn("month", month("date"))

df.write.format("delta").mode("append").partitionBy("year","month").save("/Volumes/delta_practice_catalog/test_schema/users/sales")

# Alternative: Save as managed table
df.write \
    .format("delta") \
    .mode("append") \
    .partitionBy("year", "month") \
    .saveAsTable("delta_practice_catalog.test_schema.sales")
